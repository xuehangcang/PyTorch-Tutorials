{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"725D8C41C08641BD9E997007DF12C307"},"source":["Vision Transformer（ViT）是一种基于Transformer的视觉注意力模型，用于图像分类和其他计算机视觉任务。与传统卷积神经网络（CNN）不同，ViT使用自注意力机制来捕捉图像中的关键特征。该模型将输入图像分割成小块，并将每个块作为序列元素输入到Transformer编码器中。这种方法在训练时可以捕捉到图像中的局部和全局特征，从而提高了模型的性能。ViT已被证明在许多视觉任务上具有很高的准确性，并成为计算机视觉领域的热门研究方向之一。"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"949F4395FBA340AABC28DBEE9CF3BB98"},"source":["![](https://raw.githubusercontent.com/xuehangcang/DeepLearning/main/static/ViT-Transformer.png)"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"8A7BAE4E8557439DB83A203CF32BED59"},"outputs":[],"source":["import torch\n","from torch import nn\n","from torchvision import datasets\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"46A8808A373D4E41AAD781856A691E5B"},"outputs":[{"name":"stdout","output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n"]}],"source":["transform = transforms.Compose(\n","    [transforms.Resize(224),\n","     transforms.ToTensor(),\n","     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","\n","training_data = datasets.CIFAR10(root='data', train=True,download=True, transform=transform)\n","test_data = datasets.CIFAR10(root='data', train=False, download=True, transform=transform)"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"88F3A12FDAE44D44954A13DCAC232819"},"outputs":[],"source":["batch_size = 64\n","\n","train_dataloader = DataLoader(training_data, batch_size=batch_size)\n","test_dataloader = DataLoader(test_data, batch_size=batch_size)"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"84C718D6401A4B4794E4E3E581108ECC"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using cuda device\n"]}],"source":["device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n","print(f\"Using {device} device\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"4076214456674100B2258CD30DF6DD97"},"source":["## 1.PatchEmbedding "]},{"cell_type":"code","execution_count":34,"metadata":{"id":"E4F1A636A2DD4A89AC51E08DDDA30383"},"outputs":[],"source":["class PatchEmbedding(nn.Module):\n","    \"\"\"将 2 维图像转化 1 维序列的嵌入向量\"\"\"\n","\n","    def __init__(self,\n","                 in_channels: int = 3,  # 输入图像的颜色通道数，默认值为3\n","                 patch_size: int = 16,  # 将输入图像转换为卷积核的大小，默认值为16\n","                 embedding_dim: int = 768):  # 将图像转换嵌入的维度，默认值为768。\n","        super(PatchEmbedding, self).__init__()\n","        self.patch_size = patch_size\n","        # 用卷积核将图像转换为嵌入向量\n","        self.patcher = nn.Conv2d(\n","            in_channels=in_channels,\n","            out_channels=embedding_dim,\n","            kernel_size=patch_size,\n","            stride=patch_size,\n","            padding=0\n","        )\n","        # 将卷积后的图像转换为 1 维序列\n","        self.flatten = nn.Flatten(start_dim=2, end_dim=3)\n","\n","    def forward(self, x):\n","        image_resolution = x.shape[-1]  # 输入图像的分辨率\n","        assert image_resolution % self.patch_size == 0, f\"图像必须能够被卷积核整除{image_resolution},{self.patch_size}\"\n","        x_patched = self.patcher(x)\n","        x_flattened = self.flatten(x_patched)\n","        return x_flattened.permute(0, 2, 1)  # 将嵌入调整为最终维度，从 [batch_size, P^2•C, N] 转换为 [batch_size, N, P^2•C]。"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"43D221C8C1DA42AE95F437448F830F8B"},"source":["## 2.MLPBlock"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"9AAAC6E25A6A40AD9B6655827C365F5A"},"outputs":[],"source":["class MLPBlock(nn.Module):\n","    \"\"\"包含一个归一化层的多层感知器块，简称 MLP 块\"\"\"\n","\n","    def __init__(self,\n","                 embedding_dim: int = 768,  # ViT-Base 表中的隐藏大小D\n","                 mlp_size: int = 3072,  # ViT-Base 表中的 MLP 大小\n","                 dropout: float = 0.1):  # ViT-Base 表中的 MLP 的丢弃率\n","\n","        super(MLPBlock, self).__init__()\n","        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)  # 层归一化\n","        self.mlp = nn.Sequential(  # 多层感知器\n","            nn.Linear(in_features=embedding_dim, out_features=mlp_size),  # 线性变换\n","            nn.GELU(),  # GELU 激活函数\n","            nn.Dropout(p=dropout),  # 丢弃\n","            nn.Linear(in_features=mlp_size, out_features=embedding_dim),  # 线性变换\n","            nn.Dropout(p=dropout)  # 丢弃\n","        )\n","\n","    def forward(self, x):\n","        x = self.layer_norm(x)  # 归一化\n","        x = self.mlp(x)  # 多层感知器\n","        return x\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"0FD071F1FCEA4CCBB30CF27DE7FBD4B1"},"source":["## 3.MSABlock"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"BA18C03D42FC44919BB1747B463741E7"},"outputs":[],"source":["class MSABlock(nn.Module):\n","    \"\"\"创建一个多头自注意力块，简称 MSA 块 \"\"\"\n","\n","    def __init__(self,\n","                 embedding_dim: int = 768,  # ViT-Base 表中的隐藏大小D\n","                 num_heads: int = 12,  # ViT-Base 表中的头数\n","                 attn_dropout: float = 0):  # ViT-Base 表中的注意力层的丢弃率\n","\n","        super(MSABlock, self).__init__()\n","        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)  # 层归一化\n","        self.multihead_attn = nn.MultiheadAttention(  # 多头注意力\n","            embed_dim=embedding_dim,\n","            num_heads=num_heads,\n","            dropout=attn_dropout,\n","            batch_first=True\n","        )\n","\n","    def forward(self, x):\n","        x = self.layer_norm(x)\n","        attn_output, _ = self.multihead_attn(query=x, key=x, value=x, need_weights=False)\n","        return attn_output\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"444780B8FBD04134B33F27AC461D8FB9"},"source":["## 4.Encoder"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"290F46DA553742B49C0B8518360BFBC2"},"outputs":[],"source":["class Encoder(nn.Module):\n","    \"\"\"创建一个Transformer编码器块\"\"\"\n","\n","    def __init__(self,\n","                 embedding_dim: int = 768,  # ViT-Base 表中的隐藏大小D\n","                 num_heads: int = 12,  # ViT-Base 表中的头数\n","                 mlp_size: int = 3072,  # ViT-Base 表中的 MLP 大小\n","                 mlp_dropout: float = 0.1,  # ViT-Base 表中的 MLP 的丢弃率\n","                 attn_dropout: float = 0):  # ViT-Base 表中的注意力层的丢弃率\n","        super(Encoder, self).__init__()\n","        self.msa_block = MSABlock(  # 多头自注意力块\n","            embedding_dim=embedding_dim,\n","            num_heads=num_heads,\n","            attn_dropout=attn_dropout\n","        )\n","        self.mlp_block = MLPBlock(  # 多层感知器块\n","            embedding_dim=embedding_dim,\n","            mlp_size=mlp_size,\n","            dropout=mlp_dropout\n","        )\n","\n","    def forward(self, x):\n","        x = self.msa_block(x) + x  # 多头自注意力块\n","        x = self.mlp_block(x) + x  # 多层感知器块\n","        return x"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"BE9559EC27CE440081B02D4980B82707"},"source":["## 5.VisionTransformer"]},{"cell_type":"code","execution_count":38,"metadata":{"id":"2170EBE887A44F4686A0379D5811DF3D"},"outputs":[],"source":["class VisionTransformer(nn.Module):\n","    \"\"\"默认情况下使用 ViT-Base 超参数创建一个 Vision Transformer 架构\"\"\"\n","\n","    def __init__(self,\n","                 img_size: int = 224,  # ViT论文中表3的训练分辨率, 输入图像的大小，默认值为224\n","                 in_channels: int = 3,  # 输入图像通道数\n","                 patch_size: int = 16,  # 将输入图像转换为卷积核的大小，默认值为16\n","                 num_transformer_layers: int = 12,  # ViT-Base 表中的编码器层数\n","                 embedding_dim: int = 768,  # ViT-Base 表中的隐藏大小D\n","                 mlp_size: int = 3072,  # ViT-Base 表中的 MLP 大小\n","                 num_heads: int = 12,  # ViT-Base 表中的头数\n","                 attn_dropout: float = 0,  # ViT-Base 表中的注意力层的丢弃率\n","                 mlp_dropout: float = 0.1,  # ViT-Base 表中的 MLP 的丢弃率\n","                 embedding_dropout: float = 0.1,  # ViT-Base 表中的嵌入层的丢弃率\n","                 num_classes: int = 1000):  # ViT-Base 表中的分类数\n","\n","        super(VisionTransformer, self).__init__()\n","        assert img_size % patch_size == 0, f\"图像必须能够被卷积核整除，{img_size},{patch_size}\"\n","\n","        self.num_patches = (img_size * img_size) // patch_size ** 2  # 计算图像中的卷积核数量\n","        self.class_embedding = nn.Parameter(data=torch.randn(1, 1, embedding_dim),  # 类别嵌入\n","                                            requires_grad=True)\n","        self.position_embedding = nn.Parameter(data=torch.randn(1, self.num_patches + 1, embedding_dim),  # 位置嵌入\n","                                               requires_grad=True)\n","        self.embedding_dropout = nn.Dropout(p=embedding_dropout)  # 嵌入层的丢弃率\n","        self.patch_embedding = PatchEmbedding(in_channels=in_channels, patch_size=patch_size,  # 卷积嵌入\n","                                              embedding_dim=embedding_dim)\n","        self.transformer_encoder = nn.Sequential(  # 编码器\n","            *[Encoder(\n","                embedding_dim=embedding_dim,\n","                num_heads=num_heads,\n","                mlp_size=mlp_size,\n","                mlp_dropout=mlp_dropout) for _ in\n","                range(num_transformer_layers)]\n","        )\n","\n","        self.classifier = nn.Sequential(  # 分类器\n","            nn.LayerNorm(normalized_shape=embedding_dim),\n","            nn.Linear(in_features=embedding_dim,\n","                      out_features=num_classes)\n","        )\n","\n","    def forward(self, x):\n","        batch_size = x.shape[0]  # 获取批次大小\n","        class_token = self.class_embedding.expand(batch_size, -1, -1)  # 扩展类别嵌入\n","        x = self.patch_embedding(x)  # 卷积嵌入\n","        x = torch.cat((class_token, x), dim=1)  # 将类别嵌入和卷积嵌入连接\n","        x = self.position_embedding + x  # 位置嵌入\n","        x = self.embedding_dropout(x)  # 嵌入层的丢弃率\n","        x = self.transformer_encoder(x)  # 编码器\n","        x = self.classifier(x[:, 0])  # 分类器\n","        return x\n","model = VisionTransformer()\n","model = model.to(device)"]},{"cell_type":"code","execution_count":39,"metadata":{"id":"619A7A6A8EB34C8EA6D1565602CE32DB"},"outputs":[],"source":["loss_fn = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.003)"]},{"cell_type":"code","execution_count":40,"metadata":{"id":"55EF4E21A80848D6AC27719AB5DC6DA0"},"outputs":[],"source":["def train(dataloader, model, loss_fn, optimizer):\n","    \"\"\"训练\"\"\"\n","    size = len(dataloader.dataset)\n","    for batch, (X, y) in enumerate(dataloader):\n","        X = X.to(device)\n","        y = y.to(device)\n","\n","        # 计算预测和损失\n","        pred = model(X)\n","        loss = loss_fn(pred, y)\n","\n","        # 反向传播\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        if batch % 100 == 0:\n","            loss, current = loss.item(), batch * len(X)\n","            print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n"]},{"cell_type":"code","execution_count":41,"metadata":{"id":"9C67F6AA197845859DA27112CD7F9C28"},"outputs":[],"source":["def test(dataloader, model, loss_fn):\n","    \"\"\"测试\"\"\"\n","    size = len(dataloader.dataset)\n","    model.eval()\n","    test_loss, correct = 0, 0\n","\n","    with torch.no_grad():\n","        for X, y in dataloader:\n","            X = X.to(device)\n","            y = y.to(device)\n","\n","            pred = model(X)\n","            test_loss += loss_fn(pred, y).item()\n","            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n","\n","    test_loss /= size\n","    correct /= size\n","    print(f\"测试错误率: {(100 * (1 - correct)):.2f}%, 平均损失: {test_loss:>8f}\\n\")"]},{"cell_type":"code","execution_count":42,"metadata":{"id":"6411B6D49D3F4142BBD55BEBF5E4E16A"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1\n","-------------------------------\n","loss: 7.044998 [    0/50000]\n","loss: 2.129542 [ 6400/50000]\n","loss: 2.100320 [12800/50000]\n","loss: 2.206093 [19200/50000]\n","loss: 2.299979 [25600/50000]\n","loss: 2.141643 [32000/50000]\n","loss: 2.110022 [38400/50000]\n","loss: 2.037074 [44800/50000]\n","测试错误率: 79.29%, 平均损失: 0.032642\n","\n"]}],"source":["epochs = 1\n","for t in range(epochs):\n","    print(f\"Epoch {t + 1}\\n-------------------------------\")\n","    train(train_dataloader, model, loss_fn, optimizer)\n","    test(test_dataloader, model, loss_fn)"]},{"cell_type":"code","execution_count":43,"metadata":{"id":"D1DFC38DB4DC4B1BB41FD25342194578"},"outputs":[{"name":"stdout","output_type":"stream","text":["Wed Apr 26 00:25:39 2023       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 531.41                 Driver Version: 531.41       CUDA Version: 12.1     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                      TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  NVIDIA GeForce RTX 3090       WDDM | 00000000:01:00.0  On |                  N/A |\n","| 77%   60C    P2              256W / 350W|  13845MiB / 24576MiB |     75%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","|    0   N/A  N/A      8076    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe    N/A      |\n","|    0   N/A  N/A      8560    C+G   ...GeForce Experience\\NVIDIA Share.exe    N/A      |\n","|    0   N/A  N/A      9172    C+G   C:\\Windows\\explorer.exe                   N/A      |\n","|    0   N/A  N/A     13496    C+G   ...GeForce Experience\\NVIDIA Share.exe    N/A      |\n","|    0   N/A  N/A     13868    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe    N/A      |\n","|    0   N/A  N/A     13896    C+G   ...2txyewy\\StartMenuExperienceHost.exe    N/A      |\n","|    0   N/A  N/A     14560    C+G   ...1.0_x64__8wekyb3d8bbwe\\Video.UI.exe    N/A      |\n","|    0   N/A  N/A     15880    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe    N/A      |\n","|    0   N/A  N/A     17980    C+G   ...\\Docker\\frontend\\Docker Desktop.exe    N/A      |\n","|    0   N/A  N/A     23168    C+G   ...siveControlPanel\\SystemSettings.exe    N/A      |\n","|    0   N/A  N/A     26016    C+G   ...les\\Microsoft OneDrive\\OneDrive.exe    N/A      |\n","|    0   N/A  N/A     32816      C   C:\\Program Files\\Python39\\python.exe      N/A      |\n","|    0   N/A  N/A     33748    C+G   ...__8wekyb3d8bbwe\\WindowsTerminal.exe    N/A      |\n","|    0   N/A  N/A     36044    C+G   ...\\PyCharm 2022.3.2\\bin\\pycharm64.exe    N/A      |\n","|    0   N/A  N/A     39172    C+G   ...oogle\\Chrome\\Application\\chrome.exe    N/A      |\n","|    0   N/A  N/A     40028    C+G   ... VS Code\\Microsoft VS Code\\Code.exe    N/A      |\n","+---------------------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python","nbconvert_exporter":"python","file_extension":".py","version":"3.5.2","pygments_lexer":"ipython3"}},"nbformat":4,"nbformat_minor":2}